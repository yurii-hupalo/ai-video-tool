import streamlit as st
import fal_client
import openai
import os
import requests
import json
import subprocess

# –°–ø—Ä–æ–±–∞ —ñ–º–ø–æ—Ä—Ç—É mutagen
try:
    from mutagen.mp3 import MP3
except ImportError:
    st.error("üö® –ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ `mutagen` –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –í–≤–µ–¥–∏: pip install mutagen")
    MP3 = None

# --- –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø ---
st.set_page_config(page_title="AI Mega Studio", page_icon="üéõÔ∏è", layout="wide")

# ==========================================
# 1. –°–ò–°–¢–ï–ú–ù–Ü –§–£–ù–ö–¶–Ü–á (ENGINE)
# ==========================================

def check_ffmpeg():
    try:
        subprocess.run(["ffmpeg", "-version"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return True
    except FileNotFoundError:
        return False

def save_file(url, filename):
    try:
        r = requests.get(url)
        with open(filename, 'wb') as f: f.write(r.content)
        return filename
    except Exception as e:
        st.error(f"Save Error: {e}")
        return None

def get_audio_duration(filename):
    if MP3 is None: return 5
    try:
        audio = MP3(filename)
        return audio.info.length
    except Exception:
        return 5

# --- –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ê–£–î–Ü–û –¢A –°–£–ë–¢–ò–¢–†–Ü–í ---
def generate_voiceover(text, voice_name, filename):
    try:
        client = openai.OpenAI()
        response = client.audio.speech.create(
            model="tts-1",
            voice=voice_name,
            input=text
        )
        response.stream_to_file(filename)
        return filename
    except Exception as e:
        st.error(f"TTS Error: {e}")
        return None

def generate_subtitles(audio_path):
    """–ì–µ–Ω–µ—Ä—É—î SRT —Ñ–∞–π–ª —á–µ—Ä–µ–∑ Whisper"""
    try:
        client = openai.OpenAI()
        with open(audio_path, "rb") as audio_file:
            transcript = client.audio.transcriptions.create(
                model="whisper-1", file=audio_file, response_format="srt"
            )
        srt_filename = "subtitles.srt"
        with open(srt_filename, "w") as f:
            f.write(transcript)
        return os.path.abspath(srt_filename)
    except Exception as e:
        st.warning(f"Subs Error: {e}")
        return None

# --- –û–ë–†–û–ë–ö–ê –í–Ü–î–ï–û (FFMPEG) ---

def normalize_visual(input_path, output_path, duration, width, height):
    """–†–æ–±–∏—Ç—å –∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∞–±–æ –≤—ñ–¥–µ–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π MP4 —à–º–∞—Ç–æ–∫"""
    input_abs = os.path.abspath(input_path)
    output_abs = os.path.abspath(output_path)
    
    # –§—ñ–ª—å—Ç—Ä: –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è + –ü–∞–¥–∏–Ω–≥ (—â–æ–± –Ω–µ –±—É–ª–æ –ø–æ–º–∏–ª–æ–∫ —Ä–æ–∑–º—ñ—Ä—É) + –ü–∞—Ä–Ω—ñ —á–∏—Å–ª–∞
    scale = f"scale={width}:{height}:force_original_aspect_ratio=decrease,pad={width}:{height}:(ow-iw)/2:(oh-ih)/2,setsar=1"
    
    if input_path.endswith(".jpg") or input_path.endswith(".png"):
        # –ö–∞—Ä—Ç–∏–Ω–∫–∞ -> –í—ñ–¥–µ–æ (Zoom effect –¥–ª—è –¥–∏–Ω–∞–º—ñ–∫–∏)
        # zoompan=z='min(zoom+0.0015,1.5)':d=700:x='iw/2-(iw/zoom/2)':y='ih/2-(ih/zoom/2)'
        # –î–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–æ—Å—Ç—ñ –ø–æ–∫–∏ –±–µ—Ä–µ–º–æ –ø—Ä–æ—Å—Ç—É —Å—Ç–∞—Ç–∏–∫—É, –∞–ª–µ –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Ñ–æ—Ä–º–∞—Ç–æ–º
        cmd = [
            "ffmpeg", "-y", "-loop", "1", "-i", input_abs,
            "-vf", f"{scale},format=yuv420p",
            "-c:v", "libx264", "-t", str(duration), "-pix_fmt", "yuv420p", "-r", "25",
            output_abs
        ]
    else:
        # –í—ñ–¥–µ–æ -> –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–µ –í—ñ–¥–µ–æ
        cmd = [
            "ffmpeg", "-y", "-i", input_abs,
            "-vf", f"{scale},fps=25,format=yuv420p",
            "-c:v", "libx264", "-t", str(duration), "-pix_fmt", "yuv420p",
            output_abs
        ]
    subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    return output_abs

def assemble_final_video(clips, music_path, voice_path, sub_path, output_path):
    """–§—ñ–Ω–∞–ª—å–Ω–∞ —Å–∫–ª–µ–π–∫–∞ –≤—Å—å–æ–≥–æ"""
    list_file = os.path.abspath("clips.txt")
    with open(list_file, "w") as f:
        for clip in clips:
            f.write(f"file '{clip}'\n")
    
    abs_music = os.path.abspath(music_path)
    abs_voice = os.path.abspath(voice_path) if voice_path else None
    
    cmd = ["ffmpeg", "-y", "-f", "concat", "-safe", "0", "-i", list_file] # [0:v]
    
    cmd += ["-stream_loop", "-1", "-i", abs_music] # [1:a]
    
    filter_complex = ""
    if abs_voice:
        cmd += ["-i", abs_voice] # [2:a]
        # –ú—É–∑–∏–∫–∞ 10%, –ì–æ–ª–æ—Å 130% (—â–æ–± —á—ñ—Ç–∫–æ —á—É—Ç–∏)
        filter_complex = "[1:a]volume=0.1[bg];[2:a]volume=1.3[speech];[bg][speech]amix=inputs=2:duration=first[a_out]"
    else:
        filter_complex = "[1:a]volume=1.0[a_out]" # –¢—ñ–ª—å–∫–∏ –º—É–∑–∏–∫–∞

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ (—Å—Ç–∏–ª—å)
    video_filter = "null"
    if sub_path:
        sub_escaped = sub_path.replace("\\", "/").replace(":", "\\:")
        # Fontsize=16 (–¥–ª—è 9:16 –º–æ–∂–Ω–∞ –±—ñ–ª—å—à–µ), Alignment=2 (–¶–µ–Ω—Ç—Ä –∑–Ω–∏–∑—É)
        # MarginV=20 (–≤—ñ–¥—Å—Ç—É–ø –≤—ñ–¥ –Ω–∏–∑—É)
        video_filter = f"subtitles='{sub_escaped}':force_style='Fontsize=18,PrimaryColour=&Hffffff,OutlineColour=&H000000,BorderStyle=1,Outline=1,Shadow=0,Alignment=2,MarginV=50'"

    cmd += [
        "-filter_complex", f"{filter_complex};[0:v]{video_filter}[v_out]",
        "-map", "[v_out]", "-map", "[a_out]",
        "-c:v", "libx264", "-c:a", "aac", "-b:a", "192k", "-shortest",
        output_path
    ]
    
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        return output_path
    except subprocess.CalledProcessError as e:
        st.error(f"FFmpeg Error: {e}")
        return None

# ==========================================
# 2. –Ü–ù–¢–ï–†–§–ï–ô–° (SIDEBAR)
# ==========================================
with st.sidebar:
    st.title("üéõÔ∏è AI Mega Studio")
    
    openai_key = st.text_input("OpenAI Key", type="password", value="[–ê–ü–Ü –ö–õ–Æ–ß –û–ü–ï–ù–ê–Ü]")
    fal_key = st.text_input("Fal.ai Key", type="password", value="[–ê–ü–Ü –ö–õ–Æ–ß –§–ê–õ –ê–Ü]")
    
    st.markdown("---")
    
    # –ì–û–õ–û–í–ù–ò–ô –í–ò–ë–Ü–† –†–ï–ñ–ò–ú–£
    MODE = st.selectbox("–û–±–µ—Ä–∏ —Ä–µ–∂–∏–º:", [
        "üé¨ Quick Loop (Kling)", 
        "üìú Story Mode (Slideshow)", 
        "üöÄ Hybrid Pro (Video+Img+Subs)"
    ])
    
    st.markdown("---")
    
    # –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –§–û–†–ú–ê–¢–£
    format_opt = st.radio("–§–æ—Ä–º–∞—Ç:", ("9:16 Shorts", "16:9 Long"))
    if "9:16" in format_opt:
        W, H = 720, 1280
        fal_size = "portrait_16_9"
    else:
        W, H = 1280, 720
        fal_size = "landscape_16_9"
        
    # –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø –ê–£–î–Ü–û
    st.markdown("### üéµ –ê—É–¥—ñ–æ & –¢–µ–∫—Å—Ç")
    voice_opt = st.selectbox("–ì–æ–ª–æ—Å:", ["onyx", "alloy", "echo", "shimmer", "nova", "fable"])
    uploaded_music = st.file_uploader("–§–æ–Ω–æ–≤–∞ –º—É–∑–∏–∫–∞ (mp3)", type=["mp3"])
    
    # –ì–ê–õ–û–ß–ö–ê –î–õ–Ø –°–£–ë–¢–ò–¢–†–Ü–í (–¢–ï–ü–ï–† –î–õ–Ø –í–°–Ü–•)
    add_subs = st.checkbox("–î–æ–¥–∞—Ç–∏ —Å—É–±—Ç–∏—Ç—Ä–∏", value=True)

# ==========================================
# 3. –õ–û–ì–Ü–ö–ê –ü–†–û–ì–†–ê–ú–ò
# ==========================================

st.title(f"{MODE}")

if not check_ffmpeg():
    st.error("üö® –ù–µ–º–∞—î FFmpeg!")
    st.stop()
    
if "[–ê–ü–Ü" in openai_key:
    st.warning("–í—Å—Ç–∞–≤ –∫–ª—é—á—ñ!")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_key
os.environ["FAL_KEY"] = fal_key

topic = st.text_input("–¢–µ–º–∞ —Ç–≤–æ–≥–æ –≤—ñ–¥–µ–æ:", "The history of coffee")

if st.button("üöÄ –ü–û–ß–ê–¢–ò –ì–ï–ù–ï–†–ê–¶–Ü–Æ"):
    
    with st.status("üèóÔ∏è –ü—Ä–∞—Ü—é—é...", expanded=True) as status:
        
        # --- –ï–¢–ê–ü 1: –°–¶–ï–ù–ê–†–Ü–ô ---
        st.write("üìù 1. –°—Ü–µ–Ω–∞—Ä—ñ–π...")
        client = openai.OpenAI()
        
        if MODE == "üé¨ Quick Loop (Kling)":
            prompt = f"Create a looping video concept for '{topic}'. JSON: {{'visual_prompt': 'desc', 'music_mood': 'desc'}}"
        
        elif MODE == "üìú Story Mode (Slideshow)":
            prompt = f"Create a documentary script for '{topic}' with 5 scenes. JSON: {{'scenes': ['img1', 'img2'...], 'narration': 'text', 'music_mood': 'desc'}}"
            
        elif MODE == "üöÄ Hybrid Pro (Video+Img+Subs)":
            prompt = f"""
            Create a hybrid video script for '{topic}' with 4 scenes. 
            Use 'video' type only for high action, 'image' for static.
            JSON: {{
                "narration": "text", 
                "music_mood": "desc",
                "scenes": [
                    {{'type': 'image', 'prompt': '... '}},
                    {{'type': 'video', 'prompt': '... '}}
                ]
            }}
            """
            
        resp = client.chat.completions.create(model="gpt-4o", messages=[{"role":"user", "content": prompt}], response_format={"type": "json_object"})
        data = json.loads(resp.choices[0].message.content)
        
        # --- –ï–¢–ê–ü 2: –ê–£–î–Ü–û –¢–ê –°–£–ë–¢–ò–¢–†–ò ---
        st.write("üéôÔ∏è 2. –ê—É–¥—ñ–æ...")
        
        # –ì–æ–ª–æ—Å (—Ç—ñ–ª—å–∫–∏ —è–∫—â–æ —î narration)
        voice_path = None
        voice_dur = 5
        if 'narration' in data:
            voice_path = generate_voiceover(data['narration'], voice_opt, "voice.mp3")
            voice_dur = get_audio_duration(voice_path)
        
        # –ú—É–∑–∏–∫–∞
        music_path = "music.mp3"
        if uploaded_music:
            with open(music_path, "wb") as f: f.write(uploaded_music.getbuffer())
        else:
            music_len = 15 if MODE == "üé¨ Quick Loop (Kling)" else min(int(voice_dur + 5), 45)
            try:
                h_mus = fal_client.submit("fal-ai/stable-audio", arguments={"prompt": data['music_mood'], "seconds_total": music_len})
                save_file(h_mus.get()['audio_file']['url'], music_path)
            except:
                st.warning("–ú—É–∑–∏–∫–∞ –Ω–µ –≤–∏–π—à–ª–∞, –±—É–¥–µ –±–µ–∑ –Ω–µ—ó.")

        # --- –ì–ï–ù–ï–†–ê–¶–Ü–Ø –°–£–ë–¢–ò–¢–†–Ü–í (–¢–ï–ü–ï–† –î–õ–Ø –í–°–Ü–• –†–ï–ñ–ò–ú–Ü–í, –î–ï –Ñ –ì–û–õ–û–°) ---
        sub_path = None
        if add_subs and voice_path:
            st.write("üìù –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å—É–±—Ç–∏—Ç—Ä—ñ–≤ (Whisper)...")
            sub_path = generate_subtitles(voice_path)

        # --- –ï–¢–ê–ü 3: –í–Ü–ó–£–ê–õ ---
        st.write("üé® 3. –í—ñ–∑—É–∞–ª...")
        clips = []
        
        if MODE == "üé¨ Quick Loop (Kling)":
            # Kling Video
            h_img = fal_client.submit("fal-ai/flux-pro", arguments={"prompt": data['visual_prompt'], "image_size": fal_size})
            img_url = h_img.get()['images'][0]['url']
            h_vid = fal_client.submit("fal-ai/kling-video/v1/standard/image-to-video", arguments={"prompt": data['visual_prompt'], "image_url": img_url, "duration": "5"})
            vid_path = save_file(h_vid.get()['video']['url'], "raw_kling.mp4")
            clips.append(normalize_visual(vid_path, "clip_0.mp4", 10, W, H))
            
        elif MODE == "üìú Story Mode (Slideshow)":
            # Recraft Slideshow
            time_per_slide = voice_dur / len(data['scenes'])
            for i, p in enumerate(data['scenes']):
                h = fal_client.submit("fal-ai/recraft-v3", arguments={"prompt": p, "image_size": fal_size, "style": "realistic_image"})
                img_path = save_file(h.get()['images'][0]['url'], f"raw_{i}.jpg")
                clips.append(normalize_visual(img_path, f"clip_{i}.mp4", time_per_slide, W, H))
                
        elif MODE == "üöÄ Hybrid Pro (Video+Img+Subs)":
            # Mix
            time_per_scene = voice_dur / len(data['scenes'])
            for i, scene in enumerate(data['scenes']):
                prompt = scene['prompt']
                if scene['type'] == 'video':
                    h_img = fal_client.submit("fal-ai/flux-pro", arguments={"prompt": prompt, "image_size": fal_size})
                    img_url = h_img.get()['images'][0]['url']
                    h_vid = fal_client.submit("fal-ai/kling-video/v1/standard/image-to-video", arguments={"prompt": prompt, "image_url": img_url, "duration": "5"})
                    raw = save_file(h_vid.get()['video']['url'], f"raw_{i}.mp4")
                else:
                    h_img = fal_client.submit("fal-ai/recraft-v3", arguments={"prompt": prompt, "image_size": fal_size, "style": "realistic_image"})
                    raw = save_file(h_img.get()['images'][0]['url'], f"raw_{i}.jpg")
                
                clips.append(normalize_visual(raw, f"clip_{i}.mp4", time_per_scene, W, H))

        # --- –ï–¢–ê–ü 4: –ú–û–ù–¢–ê–ñ ---
        st.write("üé¨ 4. –§—ñ–Ω–∞–ª—å–Ω–∞ –∑–±—ñ—Ä–∫–∞...")
        final_file = "RESULT.mp4"
        res = assemble_final_video(clips, music_path, voice_path, sub_path, final_file)
        
        status.update(label="‚úÖ –ì–û–¢–û–í–û!", state="complete")

    if res:
        st.balloons()
        st.success(f"–†–µ–∂–∏–º: {MODE} | –§–æ—Ä–º–∞—Ç: {format_opt}")
        st.video(res)
        with open(res, "rb") as f:
            st.download_button("‚¨áÔ∏è –°–∫–∞—á–∞—Ç–∏", f, "ai_result.mp4")